#!/bin/bash

start_time=$(date +%s)

set -e


if (( cluster == 1 ))
then
  cluster=true
  eval $module_load

else
  cluster=false
fi

ulimit -s unlimited

code_dir=$(pwd)

echo "jobs: $jobs"
jobs=(${jobs})
njobs=${#jobs[@]}
nslots=(${nslots})
nx=(${nx})
ny=(${ny})
wrfv=(${wrfv})
runlog=run_${timestamp}.log
runerr=run_${timestamp}.err


si=0
pool=false
echo "JOBID ${SLURM_JOB_ID}"
if (( $njobs > 1 )) && (( pool_jobs == 1 )) && (( batch == 1 )) && [[ $job_scheduler == "sge" ]]
then
  pool=true
  qs=$(qstat -t)
  hosts=$(python ${code_dir}/pool/get_hosts.py "$JOB_ID" "$qs")
  hosts=(${hosts})
  echo "hosts ${hosts[*]}"
fi


pid=()
set +e
trap 'rc=$?' ERR

for (( i=0; i<$njobs; i++ ))
do
  cd ${run_path}/WRF_${jobs[i]}
  #include information about number of processors in namelist.input
  if (( ${nslots[i]} > 1 ))
  then
    source ${code_dir}/search_replace.sh namelist.input namelist.input 0 nproc_x ${nx[i]} nproc_y ${ny[i]}
  fi

  exec 1> run.log
  exec 2> run.err
  echo
  echo
  echo "Running job: ${jobs[i]}"   >> $runlog
  nsi=${nslots[i]}
  wrf_dir_i=${wrfv[i]}
  echo "slots: $nsi "    >> $runlog

  if (( nsi > 1 ))
  then
    echo "parallel"  >> $runlog
    if $pool
    then
      echo "si $si"  >> $runlog
      hostsi=${hosts[@]:$si:$nsi}
      echo "on hosts: $hostsi" >> $runlog

      H=$(python ${code_dir}/pool/get_hosts_set.py "$hostsi")
      python ${code_dir}/pool/rankfile.py  "${hosts[*]}" "$si" "$nsi" > rankfile.$JOB_ID
      cat rankfile.$JOB_ID

      mpiexec -H $H -rf rankfile.$JOB_ID -mca rmaps_rank_file_physical 1 -np $nsi -v -report-bindings -display-map -display-allocation ./wrf.exe & pid[i]=$!
      si=$((si + nsi))
      sleep 20
    else
      mpiexec -np $nsi -v ./wrf.exe  & pid[i]=$!
      echo "mpiexec -np $nsi -v ./wrf.exe  & pid[i]=$!"  >> $runlog
    fi
  else
    echo "serial run"  > $runlog
    ./wrf.exe & pid[i]=$!
  fi
  echo
  echo
done

sleep 10
top -u $USER -bn 1 >> ../pidlog

finished=() #finished jobs

elapsed_t=0
enforce_rtlimit=true
if [[ -z ${rtlimit} ]]
then
  enforce_rtlimit=false
  rtlimit=1
fi

#wait until processes are finished or hard time limit almost reached
while [[ ${#finished[@]} -lt $njobs ]] && [[ ${elapsed_t} -le ${rtlimit} ]]
do

  elapsed_t=$(($(date +%s) - start_time))
  
  for (( i=0; i<$njobs; i++ ))
  do
    cd ${run_path}/WRF_${jobs[i]}
    exec 1>> $runlog
    exec 2>> $runerr
    #check if process is finished and append job to finished array if it is not yet in it
    if [[ -z $(ps -o pid= -p ${pid[i]}) ]] && [[ ! " ${finished[@]} " =~ " ${i} " ]]
    then
      sleep 1
      echo  "Job WRF_${jobs[i]} finished. Elapsed time (s): ${elapsed_t}"
      finished+=($i)
    else
      #write current memory usage of all MPI processes to file
      children=$(pgrep -P ${pid[i]} -d " ")
      children_l=($children)
      if [[ ${#children_l[@]} == 1 ]]
      then
        children="$children $(pgrep -P $children -d " ")"
      fi
      all_pids_arr="${pid[i]} $children"
      all_pids_arr=(${all_pids_arr})
      all_pids=$(IFS=, ; echo "${all_pids_arr[*]}")
      echo "${jobs[i]} $all_pids" >> ../pidlog
      sum=$(top -bn 1 -p $all_pids | tail -n ${#all_pids_arr[@]}  | awk '{ printf("%-8s\n", $5) };' | paste -sd+ | bc)
      echo "${sum}/$nsi" | bc -l  >> vmemusage_VIRT_$timestamp
      sum=$(top -bn 1 -p $all_pids | tail -n ${#all_pids_arr[@]}  | awk '{ printf("%-8s\n", $6) };' | paste -sd+ | bc)
      echo "${sum}/$nsi" | bc -l  >> vmemusage_RES_$timestamp
    fi
  done
  if [[ ${enforce_rtlimit}==false ]]
  then
    elapsed_t=0
  fi
  sleep 1
done
    
#clean up
for (( i=0; i<$njobs; i++ ))
do
  cd ${run_path}/WRF_${jobs[i]}
  exec 1>> $runlog
  exec 2>> $runerr

  echo
  echo
  cat run.log
  cat run.err

  if (( ${nslots[i]} > 1 ))
  then
    cat rsl.error.0000
  fi

  echo
  
  if [[ ! " ${finished[@]} " =~ " ${i} " ]]
  then
    echo "The job is about to terminate soon! Killing wrf.exe processes..."
    kill ${pid[i]} &
  fi

  echo "job ${jobs[i]} exited with code $?"

  if (( batch == 1 ))
  then
    #save resource usage of job
    if [[ $job_scheduler == "sge" ]]
    then
      qstat -j $JOB_ID > resources_${timestamp}.info
    elif [[ $job_scheduler == "slurm" ]]
    then
      sacct -j $SLURM_JOB_ID -l -p > resources_${timestamp}.info
    fi
    #prepend log to run log
    echo -e "$(cat ${run_path}/logs/$JOB_NAME.out)\n$(cat $runlog)"
    echo -e "$(cat ${run_path}/logs/$JOB_NAME.err)\n$(cat $runerr)" >> $runerr
  fi

  if (( restart == 1 ))
  then
      echo
      echo "Concatenate output from original and restarted run"
      python -c "from run_wrf import misc_tools; misc_tools.concat_restart('$outpath', '${jobs[i]}')"
  fi
done

#echo "trap" $rc
exit ${rc}
